{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iv-B9e9qAGS"
      },
      "source": [
        "Upload the `utils.py` file [(click here)](https://drive.google.com/file/d/1kRmGjHIpIJ0Y_KDBVJSooJnPPNhUP4A-/view?usp=drive_link) to your runtime environment to use all the helper functions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "URhGxi7a2CxE",
        "outputId": "0f29aec7-ad92-4dd9-ff9f-b3719f0fe338"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8514d4a5-4363-4d06-9ab2-efde597b7c09\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8514d4a5-4363-4d06-9ab2-efde597b7c09\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving utils.py to utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2EWLoeHgqAGU"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "from utils import *\n",
        "\n",
        "# To allow evaluators to see what you got\n",
        "np.random.seed(677)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "yzkDO_hh18US"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxVM_RjhqAGV"
      },
      "source": [
        "# Homework 1 - Fully Connected Deep Networks\n",
        "\n",
        "In this exercise you will implement the components required to train a fully connected deep network from scratch using only `numpy`. To aid in this effort, we have provided you with some starter code. Your task is to fill in the missing parts of the implementation so that you can end up with a fully trainable deep network.\n",
        "\n",
        "You may complete this assignment on either Google Colab or your own machine. If you are using your own machine you will require a functioning `numpy` installation, and the `pickle` module."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtSA7ZraqAGW"
      },
      "source": [
        "## Part 1 - Implementation [60 points]\n",
        "\n",
        "In the first part of this notebook, you will complete the implementation of a `Sequential` model class that help you build deep networks and also handle the computation of derivatives through backpropagation. This part of the assignment will not deal with a specific machine learning problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": true,
        "deletable": true,
        "editable": true,
        "id": "igRiVzBIqAGW"
      },
      "outputs": [],
      "source": [
        "# superclass of modules\n",
        "class Module:\n",
        "    \"\"\"\n",
        "    Module is a super class. It could be a single layer, or a multilayer perceptron.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.train = True\n",
        "        return\n",
        "\n",
        "    def forward(self, _input):\n",
        "        \"\"\"\n",
        "        h = f(z); z is the input, and h is the output.\n",
        "\n",
        "        Inputs:\n",
        "        _input: z\n",
        "\n",
        "        Returns:\n",
        "        output h\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def backward(self, _input, d_output):\n",
        "        \"\"\"\n",
        "        Compute:\n",
        "        gradient w.r.t. _input\n",
        "        gradient w.r.t. trainable parameters\n",
        "\n",
        "        Inputs:\n",
        "        _input: z\n",
        "        _gradOutput: dL/dh\n",
        "\n",
        "        Returns:\n",
        "        gradInput: dL/dz\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def parameters(self):\n",
        "        \"\"\"\n",
        "        Return the value of trainable parameters and its corresponding gradient (Used for grandient descent)\n",
        "\n",
        "        Returns:\n",
        "        params, gradParams\n",
        "        \"\"\"\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWi0VucuqAGW"
      },
      "source": [
        "#### Forward and backward passes in the `Sequential` module [10 points]\n",
        "\n",
        "Complete the implementation of a forward and backward pass in a `Sequential` module. For now we can ignore which layers actually make up the `Sequential` module. Just treat them as abstract functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": true,
        "deletable": true,
        "editable": true,
        "id": "Y9E8HmnwqAGX"
      },
      "outputs": [],
      "source": [
        "class Sequential(Module):\n",
        "    \"\"\"\n",
        "    Sequential provides a way to plug layers together in a feed-forward manner.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        Module.__init__(self)\n",
        "        self.layers = []\n",
        "\n",
        "    def add(self, layer):\n",
        "        self.layers.append(layer) # Adding another layer at the end\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.layers)\n",
        "\n",
        "    def forward(self, _input):\n",
        "        \"\"\"\n",
        "        Feed forward through all the layers, and return the output of the last layer\n",
        "        \"\"\"\n",
        "        # self._inputs saves the input of each layer\n",
        "        # self._inputs[i] is the input of i-th layer\n",
        "        self._inputs = [_input]\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        for layer in self.layers:\n",
        "            output = layer.forward(self._inputs[-1])  #  Output from the current layer\n",
        "            self._inputs.append(output)  # Append the output to the list of inputs for the next layers\n",
        "\n",
        "        # The last element of self._inputs is the output of last layer\n",
        "        self._output = self._inputs[-1]\n",
        "        return self._output\n",
        "\n",
        "    def backward(self, _input, d_output):\n",
        "        \"\"\"\n",
        "        Backpropogate through all the layers using chain rule.\n",
        "        \"\"\"\n",
        "\n",
        "        self.d_inputs = [None] * (self.size() + 1)\n",
        "        self.d_inputs[self.size()] = d_output\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        for i in range(self.size() - 1, -1, -1):\n",
        "            d_input = self.layers[i].backward(self._inputs[i], self.d_inputs[i+1])\n",
        "            self.d_inputs[i] = d_input\n",
        "        self.d_input = self.d_inputs[0]\n",
        "        return self.d_input\n",
        "\n",
        "    def parameters(self):\n",
        "        \"\"\"\n",
        "        Return trainable parameters and its corresponding gradient in a nested list\n",
        "        \"\"\"\n",
        "        params = []\n",
        "        d_params = []\n",
        "        for m in self.layers:\n",
        "            _p, _d = m.parameters()\n",
        "            if _p is not None:\n",
        "                params.append(_p)\n",
        "                d_params.append(_d)\n",
        "        return params, d_params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7yje5BHqAGX"
      },
      "source": [
        "#### Forward and backward passes for a `FullyConnected` layer [20 points]\n",
        "\n",
        "Complete the implementation of a forward and backward pass in an affine layer. When creating the layer, also initialize the weights using a modified version of the Xavier initialization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": true,
        "deletable": true,
        "editable": true,
        "id": "HfhJB5B8qAGX"
      },
      "outputs": [],
      "source": [
        "class FullyConnected(Module):\n",
        "    \"\"\"\n",
        "    Fully connected linear layer\n",
        "    y = Wx + b\n",
        "    \"\"\"\n",
        "    def __init__(self, inputSize, outputSize):\n",
        "        Module.__init__(self)\n",
        "\n",
        "        \"\"\"\n",
        "        Initalization: Use the Xavier initialization scheme to\n",
        "        set the standard deviation of the weights. We will use\n",
        "        the uniform distribution instead of the normal distribution\n",
        "        in order to avoid large weight values. Sometimes practitioners\n",
        "        also use the truncated normal distribution for this purpose.\n",
        "        \"\"\"\n",
        "\n",
        "        stdv = np.sqrt(2.0 / (inputSize + outputSize)) # YOUR CODE HERE\n",
        "\n",
        "        self.weight = np.random.uniform(-stdv, stdv, (outputSize, inputSize))\n",
        "        self.d_weight = np.zeros((outputSize, inputSize))\n",
        "        self.bias = np.random.uniform(-stdv, stdv, outputSize)\n",
        "        self.d_bias = np.zeros(outputSize)\n",
        "\n",
        "    def forward(self, _input):\n",
        "        \"\"\"\n",
        "\n",
        "        _input:\n",
        "        N x inputSize matrix\n",
        "\n",
        "        \"\"\"\n",
        "        self._output = np.dot(_input, self.weight.T) + self.bias # YOUR CODE HERE\n",
        "        return self._output\n",
        "\n",
        "    def backward(self, _input, d_out):\n",
        "        \"\"\"\n",
        "        _input:\n",
        "        N x inputSize matrix\n",
        "        d_out:\n",
        "        N x outputSize matrix\n",
        "        \"\"\"\n",
        "        self.d_weight = np.dot(d_out.T, _input) # YOUR CODE HERE\n",
        "        self.d_bias = np.sum(d_out, axis=0) # YOUR CODE HERE\n",
        "\n",
        "        self.d_input = np.dot(d_out, self.weight) # YOUR CODE HERE\n",
        "        return self.d_input\n",
        "\n",
        "    def parameters(self):\n",
        "        \"\"\"\n",
        "        Return weight and bias and their g\n",
        "        \"\"\"\n",
        "        return [self.weight, self.bias], [self.d_weight, self.d_bias]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlQCnZ7pqAGY"
      },
      "source": [
        "#### Forward and backward passes for a `ReLU` layer [10 points]\n",
        "\n",
        "Complete the implementation of a forward and backward pass in a `ReLU` layer. This layer does not have any parameters. If you are so inclined, you may use this template to also implement `Sigmoid` and `Tanh` nonlinearity layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": true,
        "deletable": true,
        "editable": true,
        "id": "8-9fhAzdqAGY"
      },
      "outputs": [],
      "source": [
        "class ReLU(Module):\n",
        "    \"\"\"\n",
        "    ReLU activation, no trainable parameters\n",
        "    y = max(x, 0)\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        Module.__init__(self)\n",
        "        return\n",
        "\n",
        "    def forward(self, _input):\n",
        "        \"\"\"\n",
        "        _input:\n",
        "        N x d matrix\n",
        "        \"\"\"\n",
        "        self._output = np.maximum(0, _input) # YOUR CODE HERE\n",
        "        return self._output\n",
        "\n",
        "    def backward(self, _input, d_out):\n",
        "        \"\"\"\n",
        "        _input:\n",
        "        N x d matrix\n",
        "\n",
        "        d_out:\n",
        "        N x d matrix\n",
        "        \"\"\"\n",
        "        self.d_input = d_out * (_input > 0) # YOUR CODE HERE\n",
        "        return self.d_input\n",
        "\n",
        "    def parameters(self):\n",
        "        \"\"\"\n",
        "        No trainable parameters, return None\n",
        "        \"\"\"\n",
        "        return None, None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9HKcfljqAGY"
      },
      "source": [
        "#### `SoftMax` loss with gradient [20 points]\n",
        "\n",
        "Complete the implementation of a forward and backward pass for the `SoftMaxLoss`. Remember to take numerical stability issues seriously when computing the Softmax function. Large and small logit values can create issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": true,
        "deletable": true,
        "editable": true,
        "id": "k8sz_KwDqAGY"
      },
      "outputs": [],
      "source": [
        "class SoftMaxLoss:\n",
        "    def __init__(self):\n",
        "        return\n",
        "\n",
        "    def forward(self, _input, _label):\n",
        "        \"\"\"\n",
        "        Softmax and cross entropy loss layer. Should return a scalar, since it's a\n",
        "        loss.\n",
        "\n",
        "        _input: N x C\n",
        "        _labels: N x C, one-hot\n",
        "\n",
        "        Returns: loss (scalar)\n",
        "        \"\"\"\n",
        "        #YOUR CODE HERE\n",
        "        shift_input = _input - np.max(_input, axis=1, keepdims=True)\n",
        "\n",
        "        # Softmax function\n",
        "        exps = np.exp(shift_input)\n",
        "        softmax_output = exps / np.sum(exps, axis=1, keepdims=True)\n",
        "\n",
        "        # Cross-entropy loss\n",
        "        self._output = -np.sum(_label * np.log(softmax_output + 1e-12)) / _input.shape[0]\n",
        "\n",
        "        self.softmax_output = softmax_output\n",
        "\n",
        "        return self._output\n",
        "\n",
        "    def backward(self, _input, _label):\n",
        "        self.d_input = (self.softmax_output - _label) / _input.shape[0] # YOUR CODE HERE\n",
        "        return self.d_input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9Mqzf-nqAGY"
      },
      "source": [
        "### Testing against Numerical Gradients\n",
        "\n",
        "Now we can test our implementation. To do this we use the `numeric_gradient` function in `utils.py`. The numeric gradient is estimated as follows (where $e_i$ are basis vectors):\n",
        "\n",
        "$$ [\\nabla f(x)]_i = \\frac{f(x+\\epsilon e_i) - f(x- \\epsilon e_i)}{2 \\epsilon} $$\n",
        "\n",
        "As long as the gradient we compute through backprop is close to the numerical gradient, we can be confident that our implementation will work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "BIsRmmAjqAGY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f16b0fdc-1504-4e8e-e5ff-c879380fcb2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.966419986814647e-09\n"
          ]
        }
      ],
      "source": [
        "# Test softmaxloss, the relative error should be small enough\n",
        "def test_sm():\n",
        "    crit = SoftMaxLoss()\n",
        "    gt = np.zeros((3, 10))\n",
        "    gt[np.arange(3), np.array([1,2,3])] = 1\n",
        "    x = np.random.random((3,10))\n",
        "    def test_f(x):\n",
        "        return crit.forward(x, gt)\n",
        "\n",
        "    crit.forward(x, gt)\n",
        "\n",
        "    d_input = crit.backward(x, gt)\n",
        "    d_input_numeric = numeric_gradient(test_f, x, 1, 1e-6)\n",
        "    # print(d_input)\n",
        "    # print(d_input_numeric)\n",
        "    print(relative_error(d_input, d_input_numeric, 1e-8))\n",
        "\n",
        "test_sm()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "DD9U9Qc4qAGZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a23ea583-27fa-4c36-b9db-00dff73f5e53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7.312466155746781e-08\n",
            "5.962448038856628e-10\n",
            "3.2784330387251717e-09\n"
          ]
        }
      ],
      "source": [
        "# Test modules, all the relative errors should be small enough\n",
        "def test_module(model):\n",
        "\n",
        "    # model.evaluate()\n",
        "\n",
        "    crit = TestCriterion()\n",
        "    gt = np.random.random((3,10))\n",
        "    x = np.random.random((3,10))\n",
        "    def test_f(x):\n",
        "        return crit.forward(model.forward(x), gt)\n",
        "\n",
        "    d_input = model.backward(x, crit.backward(model.forward(x), gt))\n",
        "    d_input_numeric = numeric_gradient(test_f, x, 1, 1e-6)\n",
        "    print(relative_error(d_input, d_input_numeric, 1e-8))\n",
        "\n",
        "# Test fully connected\n",
        "model = FullyConnected(10, 10)\n",
        "test_module(model)\n",
        "\n",
        "# Test ReLU\n",
        "model = ReLU()\n",
        "test_module(model)\n",
        "\n",
        "# Test Sequential\n",
        "model = Sequential()\n",
        "model.add(FullyConnected(10, 10))\n",
        "model.add(ReLU())\n",
        "test_module(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FywurzcLqAGZ"
      },
      "source": [
        "Finally we perform a sanity check for the `sgd` optimizer provided in `utils.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "poTC4mgDqAGZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e019103-54b7-4bdd-bcf5-80554c1724a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6274186902506809\n",
            "0.01806809817803746\n",
            "0.006363642344207082\n",
            "0.006242982311460435\n",
            "0.006124830766812822\n",
            "0.006009135559503139\n",
            "0.005895845622969916\n",
            "0.005784910952311122\n",
            "0.005676282582212528\n",
            "0.00556991256533503\n",
            "0.005465753951151185\n"
          ]
        }
      ],
      "source": [
        "# Test gradient descent, the loss should be lower and lower\n",
        "trainX = np.random.random((10,5))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(FullyConnected(5, 3))\n",
        "model.add(ReLU())\n",
        "model.add(FullyConnected(3, 1))\n",
        "\n",
        "crit = TestCriterion()\n",
        "\n",
        "it = 0\n",
        "state = None\n",
        "while True:\n",
        "    output = model.forward(trainX)\n",
        "    loss = crit.forward(output, None)\n",
        "    if it % 100 == 0:\n",
        "        print(loss)\n",
        "    doutput = crit.backward(output, None)\n",
        "    model.backward(trainX, doutput)\n",
        "    params, gradParams = model.parameters()\n",
        "    sgd(params, gradParams, 0.01, 0.8)\n",
        "    if it > 1000:\n",
        "        break\n",
        "    it += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "_lPuwfrhqAGZ"
      },
      "source": [
        "## Part 2 - Test on Real Data [20 points]\n",
        "\n",
        "In this section we will test the machinery we have built on a \"real world\" image categorization problem. The data we will be using is a subset of the Fashion MNIST dataset. The `pickle` file containing this data set can be downloaded [here](https://www.dropbox.com/scl/fi/mxxg0k6x8q17feef2bvwg/fashion_mnist_small.pkl?rlkey=kb2j5zfqs6qen19jxs2f5g69z&dl=0). Put the data file in a convenient location, and make sure you include the path in the cell below.\n",
        "\n",
        "The dataset contains a training set of $8000$ examples, validation set of $2000$ examples, and test set of $1000$ examples. The data consists of small $28 \\times 28$ images that have been centered, scaled, and flattened into $784$ dimensional vectors with zero mean and unit variance."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "BaIGkiO22XPL",
        "outputId": "34c955ce-458a-45b6-9372-1424df2dc6c2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c5e33983-85c0-4994-9316-ecf63db85243\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c5e33983-85c0-4994-9316-ecf63db85243\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving fashion_mnist_small.pkl to fashion_mnist_small.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "GEjtVOT3qAGZ"
      },
      "outputs": [],
      "source": [
        "filename = 'fashion_mnist_small.pkl'\n",
        "X_train, y_train, X_val, y_val, X_test, y_test = load_fmnist_data(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXSJao1rqAGZ"
      },
      "source": [
        "Write a function to build your Fully connected deep networks so that you can experiment with different hidden layer sizes and number of layers quickly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "EgLN6gRHqAGZ"
      },
      "outputs": [],
      "source": [
        "def build_model(input_size, hidden_sizes, output_size, activation_func='ReLU'):\n",
        "    \"\"\"\n",
        "    Build the model:\n",
        "    input_size: the dimension of input data\n",
        "    hidden_sizes: a list with the sizes of each hidden layer\n",
        "    output_size: the size of the output layer\n",
        "    activation_func: the activation function to use ('ReLU', 'Logistic', 'Tanh', etc.)\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "\n",
        "    # Add the first hidden layer connected to the input\n",
        "    model.add(FullyConnected(input_size, hidden_sizes[0]))\n",
        "\n",
        "    # Add the activation function after the first hidden layer\n",
        "    if activation_func == 'ReLU':\n",
        "        model.add(ReLU())\n",
        "    elif activation_func == 'Logistic':\n",
        "        model.add(Logistic())\n",
        "    elif activation_func == 'Tanh':\n",
        "        model.add(Tanh())\n",
        "\n",
        "\n",
        "    # Add any additional hidden layers\n",
        "    for i in range(1, len(hidden_sizes)):\n",
        "        model.add(FullyConnected(hidden_sizes[i-1], hidden_sizes[i]))\n",
        "\n",
        "        # Add the activation function after each hidden layer\n",
        "        if activation_func == 'ReLU':\n",
        "            model.add(ReLU())\n",
        "        elif activation_func == 'Logistic':\n",
        "            model.add(Logistic())\n",
        "        elif activation_func == 'Tanh':\n",
        "            model.add(Tanh())\n",
        "\n",
        "\n",
        "    # Output layer\n",
        "    model.add(FullyConnected(hidden_sizes[-1], output_size))\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKWee-N1qAGZ"
      },
      "source": [
        "Train your fully connected deep network models using the training data. You can use the validation data for hyperparameter tuning. One suggested hyperparameter is `weight decay` ($\\lambda$) but you may also experiment with different choices for the model architecture. We will use the `train` function from `utils.py` to conduct the training. You may want to read through it and familiarize yourself with the code to fix bugs that may arise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "deletable": true,
        "editable": true,
        "scrolled": false,
        "id": "KPmYWeQUqAGZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31825b8d-2169-4f00-b046-f6232ec674c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       0 batch loss: 2.306 train error: 89.050 val error: 87.900\n",
            "     500 batch loss: 2.084 train error: 56.725 val error: 56.900\n",
            "    1000 batch loss: 1.819 train error: 42.437 val error: 43.950\n",
            "    1500 batch loss: 1.475 train error: 41.138 val error: 42.850\n",
            "    2000 batch loss: 1.300 train error: 38.887 val error: 40.100\n",
            "    2500 batch loss: 1.259 train error: 35.638 val error: 36.100\n",
            "    3000 batch loss: 1.128 train error: 32.875 val error: 33.000\n",
            "    3500 batch loss: 0.885 train error: 30.500 val error: 31.800\n",
            "    4000 batch loss: 0.896 train error: 29.038 val error: 30.400\n",
            "    4500 batch loss: 0.945 train error: 27.975 val error: 29.750\n",
            "    5000 batch loss: 0.901 train error: 26.962 val error: 28.300\n",
            "    5500 batch loss: 0.711 train error: 26.363 val error: 27.800\n",
            "    6000 batch loss: 0.748 train error: 25.900 val error: 27.500\n",
            "    6500 batch loss: 0.846 train error: 25.575 val error: 27.300\n",
            "    7000 batch loss: 0.838 train error: 25.138 val error: 26.850\n",
            "    7500 batch loss: 0.659 train error: 24.825 val error: 26.750\n",
            "    8000 batch loss: 0.682 train error: 24.425 val error: 26.550\n",
            "    8500 batch loss: 0.791 train error: 24.287 val error: 26.350\n",
            "    9000 batch loss: 0.789 train error: 23.887 val error: 26.250\n",
            "    9500 batch loss: 0.624 train error: 23.513 val error: 25.900\n",
            "   10000 batch loss: 0.634 train error: 23.313 val error: 25.500\n",
            "   10500 batch loss: 0.754 train error: 23.275 val error: 25.250\n",
            "   11000 batch loss: 0.758 train error: 23.050 val error: 25.150\n",
            "   11500 batch loss: 0.608 train error: 22.950 val error: 25.100\n",
            "   12000 batch loss: 0.614 train error: 22.900 val error: 25.000\n",
            "   12500 batch loss: 0.737 train error: 22.837 val error: 24.950\n",
            "   13000 batch loss: 0.740 train error: 22.775 val error: 24.900\n",
            "   13500 batch loss: 0.596 train error: 22.637 val error: 24.950\n",
            "   14000 batch loss: 0.596 train error: 22.550 val error: 24.950\n",
            "   14500 batch loss: 0.722 train error: 22.425 val error: 24.900\n",
            "   15000 batch loss: 0.723 train error: 22.425 val error: 24.800\n",
            "   15500 batch loss: 0.588 train error: 22.325 val error: 24.800\n",
            "   16000 batch loss: 0.584 train error: 22.300 val error: 24.750\n",
            "   16500 batch loss: 0.713 train error: 22.288 val error: 24.700\n",
            "   17000 batch loss: 0.716 train error: 22.250 val error: 24.650\n",
            "   17500 batch loss: 0.583 train error: 22.100 val error: 24.550\n",
            "   18000 batch loss: 0.576 train error: 22.062 val error: 24.600\n",
            "   18500 batch loss: 0.707 train error: 22.037 val error: 24.600\n",
            "   19000 batch loss: 0.708 train error: 21.962 val error: 24.500\n",
            "   19500 batch loss: 0.578 train error: 21.925 val error: 24.400\n",
            "Lambda=0.0000, Train Error=24.40%, Validation Error=21.92%\n",
            "       0 batch loss: 2.292 train error: 89.900 val error: 89.500\n",
            "     500 batch loss: 2.069 train error: 53.450 val error: 54.700\n",
            "    1000 batch loss: 1.814 train error: 48.150 val error: 48.050\n",
            "    1500 batch loss: 1.516 train error: 46.462 val error: 45.800\n",
            "    2000 batch loss: 1.258 train error: 40.188 val error: 40.050\n",
            "    2500 batch loss: 1.197 train error: 34.200 val error: 34.550\n",
            "    3000 batch loss: 1.033 train error: 31.188 val error: 31.850\n",
            "    3500 batch loss: 0.960 train error: 29.175 val error: 30.050\n",
            "    4000 batch loss: 0.793 train error: 28.050 val error: 28.850\n",
            "    4500 batch loss: 0.861 train error: 27.012 val error: 28.100\n",
            "    5000 batch loss: 0.798 train error: 25.838 val error: 27.500\n",
            "    5500 batch loss: 0.790 train error: 25.362 val error: 27.550\n",
            "    6000 batch loss: 0.666 train error: 25.162 val error: 27.150\n",
            "    6500 batch loss: 0.768 train error: 24.913 val error: 26.700\n",
            "    7000 batch loss: 0.740 train error: 24.450 val error: 26.300\n",
            "    7500 batch loss: 0.732 train error: 24.187 val error: 25.800\n",
            "    8000 batch loss: 0.618 train error: 24.038 val error: 25.650\n",
            "    8500 batch loss: 0.717 train error: 23.862 val error: 25.600\n",
            "    9000 batch loss: 0.698 train error: 23.525 val error: 25.450\n",
            "    9500 batch loss: 0.689 train error: 23.275 val error: 25.100\n",
            "   10000 batch loss: 0.584 train error: 23.150 val error: 24.700\n",
            "   10500 batch loss: 0.684 train error: 22.938 val error: 24.600\n",
            "   11000 batch loss: 0.674 train error: 22.837 val error: 24.450\n",
            "   11500 batch loss: 0.667 train error: 22.775 val error: 24.450\n",
            "   12000 batch loss: 0.570 train error: 22.700 val error: 24.350\n",
            "   12500 batch loss: 0.668 train error: 22.613 val error: 24.250\n",
            "   13000 batch loss: 0.660 train error: 22.513 val error: 24.100\n",
            "   13500 batch loss: 0.651 train error: 22.362 val error: 23.850\n",
            "   14000 batch loss: 0.558 train error: 22.200 val error: 23.750\n",
            "   14500 batch loss: 0.653 train error: 22.088 val error: 23.750\n",
            "   15000 batch loss: 0.648 train error: 22.037 val error: 23.650\n",
            "   15500 batch loss: 0.638 train error: 22.013 val error: 23.650\n",
            "   16000 batch loss: 0.550 train error: 21.975 val error: 23.600\n",
            "   16500 batch loss: 0.645 train error: 21.900 val error: 23.650\n",
            "   17000 batch loss: 0.643 train error: 21.825 val error: 23.700\n",
            "   17500 batch loss: 0.632 train error: 21.837 val error: 23.650\n",
            "   18000 batch loss: 0.545 train error: 21.850 val error: 23.700\n",
            "   18500 batch loss: 0.639 train error: 21.812 val error: 23.650\n",
            "   19000 batch loss: 0.637 train error: 21.713 val error: 23.650\n",
            "   19500 batch loss: 0.625 train error: 21.750 val error: 23.650\n",
            "Lambda=0.0010, Train Error=23.60%, Validation Error=21.75%\n",
            "       0 batch loss: 2.351 train error: 93.987 val error: 94.150\n",
            "     500 batch loss: 2.063 train error: 61.312 val error: 62.500\n",
            "    1000 batch loss: 1.803 train error: 38.550 val error: 39.050\n",
            "    1500 batch loss: 1.486 train error: 32.862 val error: 33.800\n",
            "    2000 batch loss: 1.173 train error: 31.075 val error: 31.700\n",
            "    2500 batch loss: 1.128 train error: 29.138 val error: 29.400\n",
            "    3000 batch loss: 0.966 train error: 27.338 val error: 28.100\n",
            "    3500 batch loss: 0.882 train error: 26.450 val error: 27.700\n",
            "    4000 batch loss: 0.715 train error: 25.813 val error: 27.050\n",
            "    4500 batch loss: 0.800 train error: 25.162 val error: 26.150\n",
            "    5000 batch loss: 0.722 train error: 24.538 val error: 25.300\n",
            "    5500 batch loss: 0.735 train error: 24.275 val error: 25.200\n",
            "    6000 batch loss: 0.613 train error: 24.200 val error: 25.250\n",
            "    6500 batch loss: 0.717 train error: 24.038 val error: 24.850\n",
            "    7000 batch loss: 0.670 train error: 23.700 val error: 24.800\n",
            "    7500 batch loss: 0.690 train error: 23.575 val error: 24.650\n",
            "    8000 batch loss: 0.576 train error: 23.337 val error: 24.550\n",
            "    8500 batch loss: 0.676 train error: 23.175 val error: 24.400\n",
            "    9000 batch loss: 0.637 train error: 22.962 val error: 24.400\n",
            "    9500 batch loss: 0.657 train error: 22.738 val error: 24.400\n",
            "   10000 batch loss: 0.550 train error: 22.613 val error: 24.250\n",
            "   10500 batch loss: 0.651 train error: 22.500 val error: 24.200\n",
            "   11000 batch loss: 0.619 train error: 22.337 val error: 24.000\n",
            "   11500 batch loss: 0.639 train error: 22.225 val error: 23.850\n",
            "   12000 batch loss: 0.539 train error: 22.162 val error: 23.750\n",
            "   12500 batch loss: 0.639 train error: 22.075 val error: 23.650\n",
            "   13000 batch loss: 0.609 train error: 21.913 val error: 23.550\n",
            "   13500 batch loss: 0.627 train error: 21.812 val error: 23.650\n",
            "   14000 batch loss: 0.530 train error: 21.775 val error: 23.500\n",
            "   14500 batch loss: 0.628 train error: 21.675 val error: 23.500\n",
            "   15000 batch loss: 0.600 train error: 21.550 val error: 23.400\n",
            "   15500 batch loss: 0.616 train error: 21.513 val error: 23.250\n",
            "   16000 batch loss: 0.523 train error: 21.488 val error: 23.250\n",
            "   16500 batch loss: 0.622 train error: 21.425 val error: 23.300\n",
            "   17000 batch loss: 0.596 train error: 21.400 val error: 23.250\n",
            "   17500 batch loss: 0.611 train error: 21.337 val error: 23.350\n",
            "   18000 batch loss: 0.519 train error: 21.300 val error: 23.250\n",
            "   18500 batch loss: 0.617 train error: 21.250 val error: 23.350\n",
            "   19000 batch loss: 0.592 train error: 21.175 val error: 23.300\n",
            "   19500 batch loss: 0.606 train error: 21.150 val error: 23.300\n",
            "Lambda=0.0100, Train Error=23.25%, Validation Error=21.15%\n",
            "       0 batch loss: 2.352 train error: 87.587 val error: 86.400\n",
            "     500 batch loss: 2.072 train error: 60.775 val error: 59.450\n",
            "    1000 batch loss: 1.775 train error: 44.812 val error: 45.850\n",
            "    1500 batch loss: 1.554 train error: 39.538 val error: 40.850\n",
            "    2000 batch loss: 1.366 train error: 35.700 val error: 35.550\n",
            "    2500 batch loss: 1.171 train error: 32.625 val error: 32.450\n",
            "    3000 batch loss: 0.946 train error: 30.350 val error: 29.850\n",
            "    3500 batch loss: 0.922 train error: 28.338 val error: 28.450\n",
            "    4000 batch loss: 0.951 train error: 27.025 val error: 27.600\n",
            "    4500 batch loss: 0.845 train error: 26.150 val error: 27.050\n",
            "    5000 batch loss: 0.713 train error: 25.225 val error: 26.000\n",
            "    5500 batch loss: 0.739 train error: 24.813 val error: 25.500\n",
            "    6000 batch loss: 0.836 train error: 24.700 val error: 25.400\n",
            "    6500 batch loss: 0.761 train error: 24.438 val error: 25.200\n",
            "    7000 batch loss: 0.660 train error: 24.187 val error: 25.150\n",
            "    7500 batch loss: 0.679 train error: 23.925 val error: 24.900\n",
            "    8000 batch loss: 0.787 train error: 23.762 val error: 24.750\n",
            "    8500 batch loss: 0.718 train error: 23.587 val error: 24.650\n",
            "    9000 batch loss: 0.624 train error: 23.438 val error: 24.650\n",
            "    9500 batch loss: 0.638 train error: 23.400 val error: 24.650\n",
            "   10000 batch loss: 0.748 train error: 23.238 val error: 24.800\n",
            "   10500 batch loss: 0.689 train error: 23.213 val error: 24.600\n",
            "   11000 batch loss: 0.603 train error: 23.113 val error: 24.450\n",
            "   11500 batch loss: 0.617 train error: 22.988 val error: 24.350\n",
            "   12000 batch loss: 0.732 train error: 22.913 val error: 24.150\n",
            "   12500 batch loss: 0.677 train error: 22.738 val error: 24.050\n",
            "   13000 batch loss: 0.592 train error: 22.588 val error: 23.950\n",
            "   13500 batch loss: 0.603 train error: 22.513 val error: 23.800\n",
            "   14000 batch loss: 0.717 train error: 22.462 val error: 23.800\n",
            "   14500 batch loss: 0.665 train error: 22.337 val error: 23.800\n",
            "   15000 batch loss: 0.583 train error: 22.237 val error: 23.800\n",
            "   15500 batch loss: 0.592 train error: 22.275 val error: 23.800\n",
            "   16000 batch loss: 0.707 train error: 22.312 val error: 23.800\n",
            "   16500 batch loss: 0.659 train error: 22.213 val error: 23.700\n",
            "   17000 batch loss: 0.578 train error: 22.188 val error: 23.650\n",
            "   17500 batch loss: 0.586 train error: 22.137 val error: 23.650\n",
            "   18000 batch loss: 0.700 train error: 22.088 val error: 23.650\n",
            "   18500 batch loss: 0.654 train error: 22.050 val error: 23.650\n",
            "   19000 batch loss: 0.574 train error: 22.050 val error: 23.650\n",
            "   19500 batch loss: 0.580 train error: 21.988 val error: 23.700\n",
            "Lambda=0.1000, Train Error=23.65%, Validation Error=21.99%\n"
          ]
        }
      ],
      "source": [
        "def one_hot_encode(labels, num_classes):\n",
        "    \"\"\"Convert a 1D array of class labels to a 2D one-hot encoded matrix.\"\"\"\n",
        "    return np.eye(num_classes)[labels]\n",
        "\n",
        "# Load the Fashion MNIST dataset\n",
        "filename = 'fashion_mnist_small.pkl'\n",
        "X_train, y_train, X_val, y_val, X_test, y_test = load_fmnist_data(filename)\n",
        "\n",
        "# Ensure y_train and y_val are 2-dimensional (one-hot encoded)\n",
        "NCLASSES = 10  # Assuming y_train contains labels 0 through NCLASSES-1\n",
        "y_train_one_hot = one_hot_encode(y_train, NCLASSES)\n",
        "y_val_one_hot = one_hot_encode(y_val, NCLASSES)\n",
        "\n",
        "# Training options setup\n",
        "trainopt = {\n",
        "    'lr': .001,  # Initial learning rate\n",
        "    'maxiter': 20000,  # Max number of iterations (updates) of SGD\n",
        "    'display_iter': 500,  # Display batch loss every display_iter updates\n",
        "    'batch_size': 100,\n",
        "    'lr_decay': .5,  # When dropping lr, multiply it by this number (e.g., .5 means halve it)\n",
        "    'lr_decay_interval': .25  # The interval to apply lr decay (as a fraction of maxiter)\n",
        "}\n",
        "\n",
        "NFEATURES = X_train.shape[1]\n",
        "NCLASSES = 10\n",
        "\n",
        "\n",
        "# we will maintain a record of models trained for different values of lambda\n",
        "# these will be indexed directly by lambda value itself\n",
        "trained_models = dict()\n",
        "\n",
        "# set the (initial?) set of lambda values to explore\n",
        "lambdas = np.array([0, 0.001, 0.01, 0.1])  # CHANGE THESE VALUES IF YOU NEED TO GET BETTER PERFORMANCE\n",
        "\n",
        "hidden_sizes = [100, 50]# YOUR MODEL HERE\n",
        "\n",
        "# Training loop over different values of lambda\n",
        "for lambda_ in lambdas:\n",
        "    trainopt['weight_decay'] = lambda_  # Set current lambda as the weight decay\n",
        "    model = build_model(NFEATURES, hidden_sizes, NCLASSES)  # Build the model\n",
        "    criterion = SoftMaxLoss()  # Loss function\n",
        "\n",
        "    # -- model trained on large train set\n",
        "    trained_model, train_error, val_error = train(model, criterion, X_train, y_train_one_hot, X_val, y_val_one_hot, trainopt)\n",
        "\n",
        "    # Store the trained model and its performance metrics\n",
        "    trained_models[lambda_] = {\n",
        "        'model': trained_model,\n",
        "        'train_err': train_error,\n",
        "        'val_err': val_error\n",
        "    }\n",
        "\n",
        "    print(f'Lambda={lambda_:.4f}, Train Error={train_error:.2f}%, Validation Error={val_error:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1O0y5-cYqAGa"
      },
      "source": [
        "Choose the model and hyperparameters that have the best validation error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "lDaSzTuLqAGa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21736ff0-0521-4b90-d467-ae49383f273a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lambda= 0.0000, val error: 21.92\n",
            "lambda= 0.0010, val error: 21.75\n",
            "lambda= 0.0100, val error: 21.15\n",
            "lambda= 0.1000, val error: 21.99\n",
            "Best train model val err: 21.150000000000002\n",
            "Best train model lambda: 0.01\n"
          ]
        }
      ],
      "source": [
        "best_trained_lambda = 0.\n",
        "best_trained_model = None\n",
        "best_trained_val_err = 100.\n",
        "for lambda_, results in trained_models.items():\n",
        "    print('lambda= %.4f, val error: %.2f' %(lambda_, results['val_err']))\n",
        "    if results['val_err'] < best_trained_val_err:\n",
        "        best_trained_val_err = results['val_err']\n",
        "        best_trained_model = results['model']\n",
        "        best_trained_lambda = lambda_\n",
        "\n",
        "print(\"Best train model val err:\", best_trained_val_err)\n",
        "print(\"Best train model lambda:\", best_trained_lambda)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXAXaWKyqAGa"
      },
      "source": [
        "Evaluate your model on the heldout test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "FzDheJupqAGa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d31c4249-348c-4798-d454-69a12cc84269"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model test error: 88.8\n"
          ]
        }
      ],
      "source": [
        "test_err = 100 * error_rate(X_test, y_test, best_trained_model)\n",
        "print(\"Best model test error:\", test_err)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "deeplearning",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}